data(Sonar, package = "mlbench")
head(Sonar)
library(caret)
library(tidyverse)
library(MASS)
library(class)
library(mlbench)
set.seed(123)
fold <- createFolds(Sonar$Class, k = 10)
test.fold.sizes <- vapply(fold, length, numeric(1))
###
split.data.into.folds <- function(x, dat, n = nrow(dat)) {
which.group = factor(ifelse(seq(n) %in% x, "Test", "Training"))
split(dat, which.group)
}
#simple.sample <- split(Sonar, factor(ifelse(seq(208) %n% fold[[1]], #"Test", "Training")))
training.test <- lapply(fold, split.data.into.folds, dat = Sonar, n = nrow(Sonar))
training.test
library(caret)
library(tidyverse)
library(MASS)
library(class)
library(mlbench)
set.seed(123)
fold <- createFolds(Sonar$Class, k = 10)
test.fold.sizes <- vapply(fold, length, numeric(1))
###
split.data.into.folds <- function(x, dat, n = nrow(dat)) {
which.group = factor(ifelse(seq(n) %in% x, "Test", "Training"))
split(dat, which.group)
}
simple.sample <- split(Sonar, factor(ifelse(seq(208) %n% fold[[1]], "Test", "Training")))
library(caret)
library(tidyverse)
library(MASS)
library(class)
library(mlbench)
set.seed(123)
fold <- createFolds(Sonar$Class, k = 10)
test.fold.sizes <- vapply(fold, length, numeric(1))
###
split.data.into.folds <- function(x, dat, n = nrow(dat)) {
which.group = factor(ifelse(seq(n) %in% x, "Test", "Training"))
split(dat, which.group)
}
training.test <- lapply(fold, split.data.into.folds, dat = Sonar, n = nrow(Sonar))
data(Sonar, package = "mlbench")
head(Sonar)
library(caret)
library(tidyverse)
library(MASS)
library(class)
library(mlbench)
set.seed(123)
fold <- createFolds(Sonar$Class, k = 10)
test.fold.sizes <- vapply(fold, length, numeric(1))
###
split.data.into.folds <- function(x, dat, n = nrow(dat)) {
which.group = factor(ifelse(seq(n) %in% x, "Test", "Training"))
split(dat, which.group)
}
training.test <- lapply(fold, split.data.into.folds, dat = Sonar, n = nrow(Sonar))
predict.knn <- function(x, k = 5, label.name = "Class") {
predicted <- knn(train = x$Training %>% dplyr::select(-all_of(label.name)),
test = x$Test %>% dplyr::select(-all_of(label.name)),
cl = x$Training[[label.name]], k = k)
observed <- x$Test[[label.name]]
list(predicted = predicted, observed = observed)
}
knn.output <- lapply(training.test, predict.knn)
predict.knn <- function(x, k = 5, label.name = "Class") {
predicted <- knn(train = x$Training %>%
dplyr::select(-all_of(label.name)),
test = x$Test %>% dplyr::select(-all_of(label.name)),
cl = x$Training[[label.name]], k = k)
observed <- x$Test[[label.name]]
list(predicted = predicted, observed = observed)
}
knn.output <- lapply(training.test, predict.knn)
knn.output
x=training.test[[1]]
train=x$Traing %>% dplyr::select(-all_of("Class"))
confusion.matrix <- lapply(knn.output, function(x)
confusionMatrix(x$predicted, x$observed))
# Produce list: for each element of knn.output, apply confusionMatrix
accuracy.estimated <- vapply(confusion.matrix, function(c) c[["overall"]][["Accuracy"]], numeric(1))
metrics.byClass <- vapply(confusion.matrix, function(c) c[["byClass"]])
confusion.matrix <- lapply(knn.output, function(x)
confusionMatrix(x$predicted, x$observed))
# Produce list: for each element of knn.output, apply confusionMatrix
accuracy.estimated <- vapply(confusion.matrix, function(c) c[["overall"]][["Accuracy"]], numeric(1))
metrics.byClass <- vapply(confusion.matrix, function(c) c[["byClass"]],numeric(11))
all.metrics <- rbind(Accuracy = accuracy.estimated, metrics.byClass)
relevant.metrics <- all.metrics[row.names(all.metrics) %in% c("Accuracy", "Sensitivity", "Specificity", "F1"), ]
#Average by row
average.metrics <- apply(relevant.metrics, 1, mean)
average.metrics
m <- 100
multi.folds <- lapply(seq(m), function(i) createFolds(Sonar[["Class"]], k = 10))
k_repeatedcv <- function(fold, dat = Sonar)
{
training.test <- lapply(fold, splitDataIntoTrainTest, dat = dat, n = nrow(dat))
knn.output <- lapply(training.test, predictKnn)
confusion.matrix <- lapply(knn.output, function(x) confusionMatrix(x$predicted,
x$observed))
# Extracting Accuracy out from the output list
accuracy <- vapply(confusion.matrix, function(x) x[["overall"]][["Accuracy"]], numeric(1))
metrics <- vapply(confusion.matrix, function(x) x[["byClass"]], numeric(11))
all.metrics <- rbind(Accuracy = accuracy, metrics)
relevant <- all.metrics[row.names(all.metrics) %in% c("Accuracy", "Sensitivity", "Specificity", "F1"), ]
average.metrics <- apply(relevant, 1, mean)
return(average.metrics)
}
repeated.cv.metrics <- vapply(multi.folds, compute.CV, numeric(4))
m <- 100
multi.folds <- lapply(seq(m), function(i) createFolds(Sonar[["Class"]], k = 10))
k_repeatedcv <- function(fold, dat = Sonar)
{
training.test <- lapply(fold, splitDataIntoTrainTest, dat = dat, n = nrow(dat))
knn.output <- lapply(training.test, predictKnn)
confusion.matrix <- lapply(knn.output, function(x) confusionMatrix(x$predicted,
x$observed))
# Extracting Accuracy out from the output list
accuracy <- vapply(confusion.matrix, function(x) x[["overall"]][["Accuracy"]], numeric(1))
metrics <- vapply(confusion.matrix, function(x) x[["byClass"]], numeric(11))
all.metrics <- rbind(Accuracy = accuracy, metrics)
relevant <- all.metrics[row.names(all.metrics) %in% c("Accuracy", "Sensitivity", "Specificity", "F1"), ]
average.metrics <- apply(relevant, 1, mean)
return(average.metrics)
}
k_repeatedcv <- vapply(multi.folds, compute.CV, numeric(4))
repeated.cv.metrics <- vapply(multi.folds, k_repeatedcv, numeric(4))
m <- 100
multi.folds <- lapply(seq(m), function(i) createFolds(Sonar[["Class"]], k = 10))
k_repeatedcv <- function(fold, dat = Sonar)
{
training.test <- lapply(fold, split.data.into.folds, dat = dat, n = nrow(dat))
knn.output <- lapply(training.test, predictKnn)
confusion.matrix <- lapply(knn.output, function(x) confusionMatrix(x$predicted,
x$observed))
# Extracting Accuracy out from the output list
accuracy <- vapply(confusion.matrix, function(x) x[["overall"]][["Accuracy"]], numeric(1))
metrics <- vapply(confusion.matrix, function(x) x[["byClass"]], numeric(11))
all.metrics <- rbind(Accuracy = accuracy, metrics)
relevant <- all.metrics[row.names(all.metrics) %in% c("Accuracy", "Sensitivity", "Specificity", "F1"), ]
average.metrics <- apply(relevant, 1, mean)
return(average.metrics)
}
repeated.cv.metrics <- vapply(multi.folds, k_repeatedcv, numeric(4))
m <- 100
multi.folds <- lapply(seq(m), function(i) createFolds(Sonar[["Class"]], k = 10))
k_repeatedcv <- function(fold, dat = Sonar)
{
training.test <- lapply(fold, split.data.into.folds, dat = dat, n = nrow(dat))
knn.output <- lapply(training.test, predict.knn)
confusion.matrix <- lapply(knn.output, function(x) confusionMatrix(x$predicted,
x$observed))
# Extracting Accuracy out from the output list
accuracy <- vapply(confusion.matrix, function(x) x[["overall"]][["Accuracy"]], numeric(1))
metrics <- vapply(confusion.matrix, function(x) x[["byClass"]], numeric(11))
all.metrics <- rbind(Accuracy = accuracy, metrics)
relevant <- all.metrics[row.names(all.metrics) %in% c("Accuracy", "Sensitivity", "Specificity", "F1"), ]
average.metrics <- apply(relevant, 1, mean)
return(average.metrics)
}
repeated.cv.metrics <- vapply(multi.folds, k_repeatedcv, numeric(4))
dat <- as.data.frame(t(repeated.cv.metrics))
plot.dat <- dat %>% pivot_longer(everything(), names_to = "Metric", values_to = "Value")
ggplot(plot.dat) + geom_boxplot(aes(x = Metric, y = Value)) + theme_minimal()
